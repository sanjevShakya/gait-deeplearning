{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gait.config import pd\n",
    "from gait.utils import get_data_by_overlap_percent,get_overlap_data_all_sessions,  split_test_train_by_subjects, remove_invalid_data, get_overlap_data_all_sessions\n",
    "from gait.training import train_2dcnn_lstm_model\n",
    "from gait.evalution import save_history, save_test_history, save_accuracy_loss_figure, save_confusion_matrix_figure, compute_validations_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resultant_acceleration(X):\n",
    "    return np.sqrt(X[:,:,0] **2 + X[:,:,1] ** 2 + X[:,:,2] ** 2)\n",
    "\n",
    "def compute_resultant_gyro(X):\n",
    "    return np.sqrt(X[:,:,3] **2 + X[:,:,4] ** 2 + X[:,:,5] ** 2)\n",
    "\n",
    "def compute_resultant_angle(X):\n",
    "    return np.sqrt(X[:,:,6] **2 + X[:,:,7] ** 2 + X[:,:,8] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = '../data/'\n",
    "X_LABELS = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'p', 'y', 'r']\n",
    "SENSORS = {\n",
    "    \"LEFT\": \"LEFT\",\n",
    "    \"RIGHT\": \"RIGHT\",\n",
    "}\n",
    "SENSORS_LIST = [SENSORS[\"LEFT\"], SENSORS[\"RIGHT\"]]\n",
    "sessions = ['session2', 'session4', 'session6']\n",
    "Y_FILE = 'y_train.csv'\n",
    "X_PATH = 'data/'\n",
    "\n",
    "SUBJECT_FILE = 'subject.csv'\n",
    "# good session 2, 4, 6\n",
    "# sessions = ['session4']\n",
    "DEFAULT_SESSIONS = sessions[0]\n",
    "\n",
    "def get_X_files(label):\n",
    "    '''\n",
    "    returns X data file names\n",
    "    '''\n",
    "    return 'acc_{}_data.csv'.format(label)\n",
    "\n",
    "\n",
    "def get_data_overlap_folder(overlapPercent):\n",
    "    '''\n",
    "    returns overlapping data foldername\n",
    "    '''\n",
    "    return 'data_{}_overlap'.format(overlapPercent)\n",
    "\n",
    "\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "def load_file(filename):\n",
    "    '''\n",
    "    load data from a filename\n",
    "    '''\n",
    "    dataframe = pd.read_csv(filename, header=None,\n",
    "                            delimiter=\",\")\n",
    "    return dataframe.values\n",
    "\n",
    "\n",
    "def load_group(filenames):\n",
    "    '''\n",
    "    load data from a list of filenames\n",
    "    '''\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(name)\n",
    "        loaded.append(data)\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "\n",
    "def path_builder(session, overlapPercent, sensorName, fileName, prefix=\"\"):\n",
    "    return ROOT_DATA_DIR + session + '/' + sensorName + \"/\" + get_data_overlap_folder(overlapPercent) + '/' + prefix + fileName\n",
    "\n",
    "\n",
    "def get_unique_subjects(subjects):\n",
    "    return np.unique(subjects)\n",
    "\n",
    "\n",
    "def remove_invalid_data(X, y, subjects):\n",
    "    nan_indexes = np.argwhere(np.isnan(y))[:, 0]\n",
    "    if nan_indexes.size != 0:\n",
    "        y = np.delete(y, nan_indexes[0], axis=0)\n",
    "        subjects = np.delete(subjects, nan_indexes[0], axis=0)\n",
    "        X = np.delete(X, nan_indexes[0], axis=0)\n",
    "    return X, y, subjects\n",
    "\n",
    "\n",
    "def get_overlap_data_all_sessions(overlapPercent, xLabels=X_LABELS):\n",
    "    X_list = list()\n",
    "    y_list = list()\n",
    "    subject_list = list()\n",
    "    for session in sessions:\n",
    "        X, y, subject = get_data_by_overlap_percent(\n",
    "            overlapPercent, xLabels=X_LABELS, session=session)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        subject_list.append(subject)\n",
    "\n",
    "    return np.vstack(X_list), np.vstack(y_list), np.vstack(subject_list)\n",
    "\n",
    "\n",
    "def get_data_by_overlap_percent(overlapPercent, session, xLabels=X_LABELS):\n",
    "\n",
    "    subject_file_path_left = path_builder(session,\n",
    "                                          overlapPercent, SENSORS[\"LEFT\"], SUBJECT_FILE)\n",
    "    y_file_path_left = path_builder(\n",
    "        session, overlapPercent, SENSORS[\"LEFT\"],  Y_FILE)\n",
    "    x_files = list(map(lambda label: get_X_files(label), xLabels))\n",
    "    X_files_path_left = list(\n",
    "        map(lambda fileName: path_builder(session, overlapPercent, SENSORS[\"LEFT\"], fileName, prefix=X_PATH), x_files))\n",
    "    X_left = load_group(X_files_path_left)\n",
    "    y_left = load_file(y_file_path_left)\n",
    "    subject_left = load_file(subject_file_path_left)\n",
    "\n",
    "    subject_file_path_right = path_builder(session,\n",
    "                                           overlapPercent, SENSORS[\"RIGHT\"], SUBJECT_FILE)\n",
    "    y_file_path_right = path_builder(\n",
    "        session, overlapPercent, SENSORS[\"RIGHT\"],  Y_FILE)\n",
    "    x_files = list(map(lambda label: get_X_files(label), xLabels))\n",
    "    X_files_path_right = list(\n",
    "        map(lambda fileName: path_builder(session, overlapPercent, SENSORS[\"RIGHT\"], fileName, prefix=X_PATH), x_files))\n",
    "    X_right = load_group(X_files_path_right)\n",
    "    y_right = load_file(y_file_path_right)\n",
    "    subject_right = load_file(subject_file_path_right)\n",
    "    X = np.concatenate((X_left, X_right), axis=0)\n",
    "    y = np.concatenate((y_left, y_right), axis=0)\n",
    "    subject = np.concatenate((subject_left, subject_right), axis=0)\n",
    "\n",
    "    X, y, subject = remove_invalid_data(X, y, subject)\n",
    "    y = np.array(y, dtype=float)\n",
    "    y = np.array(y, dtype=int)\n",
    "    y = np.array(y, dtype=str)\n",
    "\n",
    "    subject = np.array(subject, dtype=str)\n",
    "    return (X, y, subject)\n",
    "\n",
    "\n",
    "def filter_excluded_subject(subjects, excluded_subjects):\n",
    "    return [subject for subject in subjects if subject not in excluded_subjects]\n",
    "\n",
    "\n",
    "def split_test_train_by_subjects(X, y, subjects, exclude_subjects=[]):\n",
    "    unique_subjects = get_unique_subjects(subjects)\n",
    "    unique_subjects = filter_excluded_subject(\n",
    "        unique_subjects, exclude_subjects)\n",
    "    np.random.shuffle(unique_subjects)\n",
    "    M = len(unique_subjects)\n",
    "\n",
    "    train_X = X[train_idx, :]\n",
    "    test_X = X[test_idx, :]\n",
    "    train_y = y[train_idx, :]\n",
    "    test_y = y[test_idx, :]\n",
    "    train_y = np.array(train_y, dtype=float)\n",
    "    test_y = np.array(test_y, dtype=float)\n",
    "    train_y = np.array(train_y, dtype=int)\n",
    "    test_y = np.array(test_y, dtype=int)\n",
    "\n",
    "    train_y = train_y\n",
    "    test_y = test_y\n",
    "    encoded_train_y = tf.keras.utils.to_categorical(train_y)\n",
    "    encoded_test_y = tf.keras.utils.to_categorical(test_y)\n",
    "\n",
    "    return train_X, test_X, encoded_train_y, encoded_test_y, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument_data(X):\n",
    "    resultant_acc = compute_resultant_acceleration(X)\n",
    "    resultant_gyro = compute_resultant_gyro(X)\n",
    "    resultant_angle = compute_resultant_angle(X)\n",
    "    resultant_acc = resultant_acc.reshape(\n",
    "        resultant_acc.shape[0], resultant_acc.shape[1], 1)\n",
    "    resultant_gyro = resultant_gyro.reshape(\n",
    "        resultant_gyro.shape[0], resultant_gyro.shape[1], 1)\n",
    "    resultant_angle = resultant_angle.reshape(\n",
    "        resultant_angle.shape[0], resultant_angle.shape[1], 1)\n",
    "    X = np.concatenate((X, resultant_acc), axis=2)\n",
    "    X = np.concatenate((X, resultant_gyro), axis=2)\n",
    "    X = np.concatenate((X, resultant_angle), axis=2)\n",
    "    return X\n",
    "\n",
    "def reshape_lstm_data(X):\n",
    "    n_steps, n_length = 4, 32\n",
    "    return X.reshape((X.shape[0], n_steps, n_length, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def get_unique_combination(arr):\n",
    "    return [com for sub in range(len(arr)) for com in combinations(arr, sub + 1)]\n",
    "\n",
    "\n",
    "def get_subjects(subjects):\n",
    "    unique_subjects = np.unique(subjects)\n",
    "    exclude_subjects = ['ddAeJA42PXvwthbW', 'nan']\n",
    "    return filter_excluded_subject(\n",
    "        unique_subjects, exclude_subjects)\n",
    "\n",
    "\n",
    "def exclude(X, y, subjects, exclude_subjects):\n",
    "    unique_subjects = get_unique_subjects(subjects)\n",
    "    unique_subjects = filter_excluded_subject(\n",
    "        unique_subjects, exclude_subjects)\n",
    "    np.random.shuffle(unique_subjects)\n",
    "    idx = np.where(subjects == unique_subjects)[0]\n",
    "    print(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "def exclude_subject_evaluate_model(model, X_t, y_t, subjects, include_list):\n",
    "    try:\n",
    "\n",
    "        idx = np.where(subjects == include_list)[0]\n",
    "        new_X_t = X_t[idx,:]\n",
    "        new_y_t = y_t[idx,:]\n",
    "        new_X_t = augument_data(new_X_t)\n",
    "        new_X_t = reshape_lstm_data(new_X_t)\n",
    "        new_y_t_en = tf.keras.utils.to_categorical(new_y_t)\n",
    "        e_h = model.evaluate(\n",
    "            new_X_t, new_y_t_en, batch_size=128, verbose=1)\n",
    "        print(e_h)\n",
    "        return e_h\n",
    "\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model('../best_model/cnn-lstm/80_overlap/best_model.75-0.05-0.9856074452400208.hdf5')\n",
    "X_t, y_t, subject_t = get_data_by_overlap_percent(0, 'session6')\n",
    "filtered_subjects = get_subjects(subject_t)\n",
    "subject_combinations = get_unique_combination(filtered_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 6ms/step - loss: 2.0626 - accuracy: 0.7060\n",
      "[2.062563896179199, 0.7060301303863525]\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5435 - accuracy: 0.8977\n",
      "[0.5434533357620239, 0.8977136015892029]\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.2431 - accuracy: 0.7622\n",
      "[1.243104100227356, 0.762150228023529]\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 1.9439 - accuracy: 0.6587\n",
      "[1.9439208507537842, 0.6586936712265015]\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.4392 - accuracy: 0.7847\n",
      "[1.4391659498214722, 0.784691333770752]\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6265 - accuracy: 0.7359\n",
      "[1.6265034675598145, 0.7358934283256531]\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2.0060 - accuracy: 0.6835\n",
      "[2.0060253143310547, 0.6834721565246582]\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.9775 - accuracy: 0.8136\n",
      "[0.9774988889694214, 0.8136135339736938]\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.3371 - accuracy: 0.7623\n",
      "[1.3371490240097046, 0.7622523307800293]\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.5547 - accuracy: 0.7162\n",
      "[1.554673671722412, 0.7161554098129272]\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 1.3605 - accuracy: 0.7756\n",
      "[1.3604627847671509, 0.7756429314613342]\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.6155 - accuracy: 0.7407\n",
      "[1.6154733896255493, 0.7406812310218811]\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.7213 - accuracy: 0.7128\n",
      "[1.7213186025619507, 0.7128332257270813]\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.2982 - accuracy: 0.7622\n",
      "[1.2981646060943604, 0.7622100114822388]\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.5023 - accuracy: 0.7472\n",
      "[1.502346396446228, 0.7472035884857178]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for combined_subjects in subject_combinations:\n",
    "    # print(list(combined_subjects))\n",
    "    history = exclude_subject_evaluate_model(saved_model,X_t, y_t, subject_t, list(combined_subjects))\n",
    "    results[combined_subjects] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('19AoxD1bgrDckd2p',) [2.062563896179199, 0.7060301303863525]\n",
      "('c9EB3mOQckRCc8Bz',) [0.5434533357620239, 0.8977136015892029]\n",
      "('csujYZktn88ftTTv',) [1.243104100227356, 0.762150228023529]\n",
      "('19AoxD1bgrDckd2p', 'c9EB3mOQckRCc8Bz') [1.4391659498214722, 0.784691333770752]\n",
      "('19AoxD1bgrDckd2p', 'csujYZktn88ftTTv') [1.6265034675598145, 0.7358934283256531]\n",
      "('19AoxD1bgrDckd2p', 'wtyNo4LYaWXrkzA7') [2.0060253143310547, 0.6834721565246582]\n",
      "('c9EB3mOQckRCc8Bz', 'csujYZktn88ftTTv') [0.9774988889694214, 0.8136135339736938]\n",
      "('c9EB3mOQckRCc8Bz', 'wtyNo4LYaWXrkzA7') [1.3371490240097046, 0.7622523307800293]\n",
      "('csujYZktn88ftTTv', 'wtyNo4LYaWXrkzA7') [1.554673671722412, 0.7161554098129272]\n",
      "('19AoxD1bgrDckd2p', 'c9EB3mOQckRCc8Bz', 'csujYZktn88ftTTv') [1.3604627847671509, 0.7756429314613342]\n",
      "('19AoxD1bgrDckd2p', 'c9EB3mOQckRCc8Bz', 'wtyNo4LYaWXrkzA7') [1.6154733896255493, 0.7406812310218811]\n",
      "('19AoxD1bgrDckd2p', 'csujYZktn88ftTTv', 'wtyNo4LYaWXrkzA7') [1.7213186025619507, 0.7128332257270813]\n",
      "('c9EB3mOQckRCc8Bz', 'csujYZktn88ftTTv', 'wtyNo4LYaWXrkzA7') [1.2981646060943604, 0.7622100114822388]\n",
      "('19AoxD1bgrDckd2p', 'c9EB3mOQckRCc8Bz', 'csujYZktn88ftTTv', 'wtyNo4LYaWXrkzA7') [1.502346396446228, 0.7472035884857178]\n"
     ]
    }
   ],
   "source": [
    "# print(results)\n",
    "\n",
    "for key in results:\n",
    "    if(results[key][1] > 0.68):    \n",
    "        print(key, results[key])\n",
    "    # print('SUBJECT: {} >>>ACC: {}'.format(key, str(value[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(subject_t))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb44b4e6cc691712e826bba4e724ba947229be6916e0acb92c0db1c9ddbd3c7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gait.config import pd\n",
    "from gait.utils import get_data_by_overlap_percent,get_overlap_data_all_sessions,  split_test_train_by_subjects, remove_invalid_data, get_overlap_data_all_sessions\n",
    "from gait.training import  cnn_final_retrain\n",
    "from gait.evalution import save_history, save_test_history, save_accuracy_loss_figure, save_confusion_matrix_figure, compute_validations_predictions,compute_validations_predictions_with_stats,compute_validations_predictions_cnn_multihead_with_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resultant_acceleration(X):\n",
    "    return np.sqrt(X[:,:,0] **2 + X[:,:,1] ** 2 + X[:,:,2] ** 2)\n",
    "\n",
    "def compute_resultant_gyro(X):\n",
    "    return np.sqrt(X[:,:,3] **2 + X[:,:,4] ** 2 + X[:,:,5] ** 2)\n",
    "\n",
    "def compute_resultant_angle(X):\n",
    "    return np.sqrt(X[:,:,6] **2 + X[:,:,7] ** 2 + X[:,:,8] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 12, 64)       3136      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 12, 64)      256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128, 12, 64)       0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 12, 64)       0         \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 42, 4, 64)        0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 42, 4, 16)         12304     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 42, 4, 16)        64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 42, 4, 16)         0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 14, 4, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 4, 16)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 896)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 7176      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,990\n",
      "Trainable params: 22,830\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model size: 89.1796875 KB\n",
      "Model Size: None\n",
      "Model saved at filepath : /home/sanjeev/thesis/code/gait-deeplearning/notebooks/../src/../models/model_0_overlap/best_model.{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1199513\n  y sizes: 11714\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c2abe6c4358f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# X_test_stats = get_statistic_feature_all_channels(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# print('Statistic feature shape: ', X_train_stats.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         model, history = cnn_final_retrain(\n\u001b[0m\u001b[1;32m     58\u001b[0m             X_train, y_train, overlap_percent=OVERLAP_PERCENT, batch_size=128, epochs=1)\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/code/gait-deeplearning/notebooks/../src/gait/training.py\u001b[0m in \u001b[0;36mcnn_final_retrain\u001b[0;34m(train_X, train_y, overlap_percent, verbose, epochs, batch_size)\u001b[0m\n\u001b[1;32m    663\u001b[0m     model.compile(loss=\"categorical_crossentropy\",\n\u001b[1;32m    664\u001b[0m                           optimizer=optimizer, metrics=[\"accuracy\"])\n\u001b[0;32m--> 665\u001b[0;31m     history = model.fit(train_X, train_y, epochs=epochs, verbose=verbose,\n\u001b[0m\u001b[1;32m    666\u001b[0m                                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1652\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1199513\n  y sizes: 11714\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "overlapPercents = [0]\n",
    "# overlapPercents = [0]\n",
    "# exclude_subjects = ['ddAeJA42PXvwthbW', 'nan', 'LLZjAPTyj7muHsEf',\n",
    "#                     'MMuX9YIh4NTbLZLM', 'cbOZWnI7s1y8oLD4', 'PE8D53oX060qLbdX', 'xYdtS1F8tDyjEIgN', 'sUZBISq61Y7I5tqQ']\n",
    "exclude_subjects = ['ddAeJA42PXvwthbW', 'nan',\n",
    "                    'MMuX9YIh4NTbLZLM',\n",
    "                    'NKdnm6cN3KKAx7R4',\n",
    "                    'PE8D53oX060qLbdX',\n",
    "                    'xYdtS1F8tDyjEIgN',\n",
    "                    'EUbKPOSQgjccjtvi',\n",
    "                    'PE8D53oX060qLbdX',\n",
    "                    'ddAeJA42PXvwthbW',\n",
    "                    'LLZjAPTyj7muHsEf',\n",
    "                    'cbOZWnI7s1y8oLD4',\n",
    "                    'ddAeJA42PXvwthbW',\n",
    "                    '1ZstYhEKzKKfGnMW',\n",
    "                    'LLZjAPTyj7muHsEf',\n",
    "                    'bQsRwKxgJiXTzo6P',\n",
    "                    'sUZBISq61Y7I5tqQ',\n",
    "                    '19AoxD1bgrDckd2p',\n",
    "                    'wtyNo4LYaWXrkzA7',\n",
    "                    ]\n",
    "\n",
    "for OVERLAP_PERCENT in overlapPercents:\n",
    "    gpus = tf.config.list_logical_devices('GPU')\n",
    "    strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "    with strategy.scope():\n",
    "        X, y, subjects = get_overlap_data_all_sessions(OVERLAP_PERCENT)\n",
    "        # REMOVE UNWANTED SUBJECTS\n",
    "        indexes = np.where(subjects == exclude_subjects)\n",
    "        X = np.delete(X, indexes[0], axis=0)\n",
    "        y = np.delete(y, indexes[0], axis=0)\n",
    "        subjects = np.delete(subjects, indexes[0], axis=0)\n",
    "        # END REMOVE UNWANTED SUBJECTS\n",
    "\n",
    "        # AUGMENT DATA\n",
    "        resultant_acc = compute_resultant_acceleration(X)\n",
    "        resultant_gyro = compute_resultant_gyro(X)\n",
    "        resultant_angle = compute_resultant_angle(X)\n",
    "        resultant_acc = resultant_acc.reshape(\n",
    "            resultant_acc.shape[0], resultant_acc.shape[1], 1)\n",
    "        resultant_gyro = resultant_gyro.reshape(\n",
    "            resultant_gyro.shape[0], resultant_gyro.shape[1], 1)\n",
    "        resultant_angle = resultant_angle.reshape(\n",
    "            resultant_angle.shape[0], resultant_angle.shape[1], 1)\n",
    "        X = np.concatenate((X, resultant_acc), axis=2)\n",
    "        X = np.concatenate((X, resultant_gyro), axis=2)\n",
    "        X = np.concatenate((X, resultant_angle), axis=2)\n",
    "        # AUGMENT DATA END\n",
    "\n",
    "        # print('UNIQUE subjects:', np.unique(subjects))\n",
    "        X_train, X_test, y_train, y_test, y_train_non_encoded, y_test_non_encoded = split_test_train_by_subjects(\n",
    "            X, y, subjects, train_percent=0.8)\n",
    "        # X_train_stats = get_statistic_feature_all_channels(X_train)\n",
    "        # X_test_stats = get_statistic_feature_all_channels(X_test)\n",
    "        # print('Statistic feature shape: ', X_train_stats.shape)\n",
    "        model, history = cnn_final_retrain(\n",
    "            X_train, y_train, overlap_percent=OVERLAP_PERCENT, batch_size=128, epochs=1)\n",
    "\n",
    "        save_history(history, OVERLAP_PERCENT)\n",
    "        evaluation_history = model.evaluate(\n",
    "            X_test, y_test, batch_size=128, verbose=1)\n",
    "        save_test_history(evaluation_history, OVERLAP_PERCENT)\n",
    "        save_accuracy_loss_figure(history, OVERLAP_PERCENT)\n",
    "        validations, predictions = compute_validations_predictions(\n",
    "            model, X_test, y_test)\n",
    "        save_confusion_matrix_figure(\n",
    "            validations, predictions, OVERLAP_PERCENT, sns)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb44b4e6cc691712e826bba4e724ba947229be6916e0acb92c0db1c9ddbd3c7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
